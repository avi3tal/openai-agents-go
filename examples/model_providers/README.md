# Model Providers Examples

This directory contains comprehensive examples demonstrating how to use various AI model providers with the OpenAI Agents Go SDK. The SDK's flexible provider architecture allows you to seamlessly integrate with multiple AI providers using their OpenAI-compatible APIs.

## ğŸ—ï¸ Provider Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Go Application â”‚â”€â”€â”€â”€â”‚   SDK Provider  â”‚
â”‚  (Agents SDK)   â”‚    â”‚   Architecture  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Single        â”‚      â”‚  Multi         â”‚      â”‚  Global        â”‚
â”‚  Provider      â”‚      â”‚  Provider      â”‚      â”‚  Default       â”‚
â”‚  (Direct)      â”‚      â”‚  (Routing)     â”‚      â”‚  (System-wide) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“ Available Examples

### ğŸ¤– [Anthropic Claude](./anthropic_claude/)
Integrate with Anthropic's Claude models using OpenAI SDK compatibility.

**Features:**
- âœ… Extended thinking mode for complex reasoning
- âœ… Advanced safety and alignment
- âœ… Large context windows (200K tokens)
- âœ… Function calling and tool use
- âœ… Multiple Claude models (Opus, Sonnet, Haiku)

**Best For:** Complex reasoning, analysis, safety-critical applications

```bash
cd anthropic_claude && go run main.go
```

### ğŸš€ [Multi-Provider](./multi_provider/)
Use multiple providers simultaneously with prefix-based routing.

**Features:**
- âœ… Prefix routing (`anthropic/`, `mistral/`, `openrouter/`)
- âœ… Provider-specific optimization
- âœ… Cost-performance analysis
- âœ… Fallback strategies
- âœ… Performance benchmarking

**Best For:** Optimization, flexibility, provider comparison

```bash
cd multi_provider && go run main.go
```

### ğŸŒ [OpenRouter Meta-Provider](./openrouter_meta/)
Access 100+ models from multiple providers through a unified API.

**Features:**
- âœ… 100+ models from various providers
- âœ… Single API key for all models
- âœ… Cost optimization and routing
- âœ… Model performance comparison
- âœ… Access to latest models

**Best For:** Model experimentation, cost control, maximum flexibility

```bash
cd openrouter_meta && go run main.go
```

### ğŸ’» [Mistral AI](./mistral_ai/)
Leverage Mistral's code-focused and multilingual capabilities.

**Features:**
- âœ… Codestral for superior code generation
- âœ… Native multilingual support
- âœ… Efficient performance-to-cost ratio
- âœ… Structured output generation
- âœ… European data sovereignty

**Best For:** Code generation, multilingual tasks, efficient inference

```bash
cd mistral_ai && go run main.go
```

### ğŸ”§ [LiteLLM](./custom_example_litellm/)
Use LiteLLM as a local proxy to multiple providers.

**Features:**
- âœ… Local Docker deployment
- âœ… Unified interface to all providers
- âœ… Cost optimization
- âœ… Automatic fallbacks
- âœ… Load balancing

**Best For:** Local development, provider abstraction, cost control

### ğŸ› ï¸ [Custom Provider](./custom_example_provider/)
Create custom provider implementations.

**Features:**
- âœ… Custom base URL and API key configuration
- âœ… Provider interface implementation
- âœ… Environment-based configuration
- âœ… Integration patterns

**Best For:** Custom endpoints, specialized providers, development testing

### âš™ï¸ [Custom Agent](./custom_example_agent/)
Configure providers at the agent level.

**Features:**
- âœ… Agent-specific provider configuration
- âœ… Per-agent model selection
- âœ… Custom client configuration
- âœ… Flexible provider switching

**Best For:** Agent-specific optimization, testing different providers

### ğŸŒ [Global Configuration](./custom_example_global/)
Set providers as system-wide defaults.

**Features:**
- âœ… Global default provider setup
- âœ… System-wide configuration
- âœ… Environment-based setup
- âœ… Simplified agent creation

**Best For:** Consistent provider usage, simplified setup

## ğŸ¯ Provider Selection Guide

### By Use Case

| Use Case | Primary Choice | Alternative | Budget Option |
|----------|---------------|-------------|---------------|
| **Complex Reasoning** | Anthropic Claude | OpenAI GPT-4 | Llama 3.1 70B (OpenRouter) |
| **Code Generation** | Mistral Codestral | Claude 3.5 Sonnet | Code Llama (OpenRouter) |
| **Multilingual** | Mistral Large | Claude | Llama 3.1 (OpenRouter) |
| **Fast Responses** | Llama 3.1 8B | Mistral Small | Gemma 2 (OpenRouter) |
| **Creative Writing** | Claude Opus | GPT-4 | Mistral Large |
| **Function Calling** | Claude | GPT-4 | Mistral Large |
| **Cost Optimization** | OpenRouter | LiteLLM | Mistral Small |

### By Performance Requirements

| Requirement | Recommended Providers | Example Models |
|-------------|----------------------|----------------|
| **Ultra-fast** | OpenRouter, Mistral | `llama-3.1-8b`, `mistral-small` |
| **Balanced** | Mistral, Multi-provider | `mistral-large`, `claude-sonnet` |
| **Maximum Quality** | Anthropic, OpenAI | `claude-opus`, `gpt-4` |
| **Code-focused** | Mistral, specialized | `codestral`, `code-llama` |
| **Multilingual** | Mistral, Claude | `mistral-large`, `claude-sonnet` |

### By Cost Sensitivity

| Budget Level | Strategy | Providers |
|--------------|----------|-----------|
| **Low Cost** | Use fast models via OpenRouter | `llama-3.1-8b`, `gemma-2-9b` |
| **Medium Cost** | Balanced models, smart routing | `mistral-large`, multi-provider |
| **High Cost** | Premium models for quality | `claude-opus`, `gpt-4` |
| **Optimized** | LiteLLM, dynamic routing | Multi-provider with fallbacks |

## ğŸ”§ Quick Setup Patterns

### 1. Single Provider (Simple)

```go
// Direct provider setup
provider := agents.NewOpenAIProvider(agents.OpenAIProviderParams{
    BaseURL: param.NewOpt("https://api.anthropic.com/v1/"),
    APIKey:  param.NewOpt("your-api-key"),
    UseResponses: param.NewOpt(false),
})

agent := agents.New("Assistant")
result, err := (agents.Runner{
    Config: agents.RunConfig{
        ModelProvider: provider,
        Model: param.NewOpt(agents.NewAgentModelName("claude-3-5-sonnet-20241022")),
    },
}).Run(context.Background(), agent, "Hello!")
```

### 2. Multi-Provider (Flexible)

```go
// Setup multiple providers with routing
providerMap := agents.NewMultiProviderMap()
providerMap.AddProvider("anthropic", anthropicProvider)
providerMap.AddProvider("mistral", mistralProvider)
providerMap.AddProvider("openrouter", openrouterProvider)

multiProvider := agents.NewMultiProvider(agents.NewMultiProviderParams{
    ProviderMap: providerMap,
})

// Use with prefixes
agent.WithModel("anthropic/claude-3-5-sonnet-20241022")
agent.WithModel("mistral/codestral-latest")
agent.WithModel("openrouter/meta-llama/llama-3.1-8b-instruct")
```

### 3. Global Default (Convenient)

```go
// Set global default
client := agents.NewOpenaiClient(
    param.NewOpt("https://api.mistral.ai/v1"),
    param.NewOpt("your-mistral-key"),
)
agents.SetDefaultOpenaiClient(client, false)
agents.SetDefaultOpenaiAPI(agents.OpenaiAPITypeChatCompletions)

// Now all agents use Mistral by default
agent := agents.New("Assistant").WithModel("mistral-large-latest")
result, err := agents.Run(context.Background(), agent, "Hello!")
```

## ğŸŒŸ Key Benefits by Provider

### **Anthropic Claude**
- ğŸ§  **Superior reasoning** and step-by-step thinking
- ğŸ›¡ï¸ **Safety-first** design with constitutional AI
- ğŸ“š **Large context** windows (200K tokens)
- ğŸ¯ **Extended thinking** mode for complex problems

### **Mistral AI**
- ğŸ’» **Code excellence** with Codestral specialization  
- ğŸŒ **Multilingual native** support (not just translation)
- âš¡ **Efficient performance** with great cost ratios
- ğŸ‡ªğŸ‡º **European sovereignty** and GDPR compliance

### **OpenRouter**
- ğŸ­ **100+ models** from all major providers
- ğŸ’° **Cost optimization** with competitive pricing
- ğŸ”„ **Easy switching** between providers and models
- ğŸ“Š **Unified analytics** across all providers

### **Multi-Provider**
- ğŸ¯ **Optimal routing** to best model for each task
- ğŸ”„ **Fallback strategies** for reliability
- ğŸ’° **Cost control** through smart model selection
- ğŸ§ª **A/B testing** capabilities

## ğŸ› ï¸ Environment Setup

### Required Environment Variables

```bash
# Anthropic Claude
export ANTHROPIC_API_KEY="sk-ant-your-key"

# Mistral AI  
export MISTRAL_API_KEY="your-mistral-key"

# OpenRouter
export OPENROUTER_API_KEY="sk-or-your-key"

# OpenAI (optional, for fallback)
export OPENAI_API_KEY="sk-your-openai-key"
```

### Development Setup

```bash
# Clone and setup
git clone <repo>
cd examples/model_providers

# Choose your example
cd anthropic_claude     # For Claude integration
cd multi_provider       # For multiple providers
cd openrouter_meta      # For OpenRouter access
cd mistral_ai          # For Mistral models

# Set API keys and run
export PROVIDER_API_KEY="your-key"
go run main.go
```

## ğŸš€ Advanced Integration Patterns

### 1. Provider Selection Strategy

```go
func selectProvider(taskType string, complexity int, budget float64) string {
    switch {
    case taskType == "code":
        return "mistral/codestral-latest"
    case complexity > 8 && budget > 0.01:
        return "anthropic/claude-3-5-sonnet-20241022"
    case budget < 0.001:
        return "openrouter/meta-llama/llama-3.1-8b-instruct"
    default:
        return "mistral/mistral-large-latest"
    }
}
```

### 2. Automatic Fallback

```go
func runWithFallback(prompt string) (string, error) {
    providers := []string{
        "anthropic/claude-3-5-sonnet-20241022",  // Primary
        "mistral/mistral-large-latest",          // Fallback 1
        "openrouter/meta-llama/llama-3.1-8b-instruct", // Fallback 2
    }
    
    for _, provider := range providers {
        result, err := runWithProvider(provider, prompt)
        if err == nil {
            return result, nil
        }
        log.Printf("Provider %s failed, trying next: %v", provider, err)
    }
    return "", errors.New("all providers failed")
}
```

### 3. Cost-Performance Optimization

```go
type ProviderMetrics struct {
    Cost         float64
    ResponseTime time.Duration
    Quality      float64
}

func optimizeProviderSelection(metrics map[string]ProviderMetrics) string {
    best := ""
    bestScore := 0.0
    
    for provider, m := range metrics {
        // Weighted score: quality important, but consider cost and speed
        score := (m.Quality * 0.6) - (m.Cost * 0.2) - (float64(m.ResponseTime.Seconds()) * 0.2)
        if score > bestScore {
            bestScore = score
            best = provider
        }
    }
    return best
}
```

## ğŸ“Š Performance Comparison

### Response Time Benchmarks
| Provider | Simple Query | Complex Reasoning | Code Generation |
|----------|-------------|------------------|-----------------|
| Mistral Small | ~0.8s | ~2.5s | ~1.8s |
| Mistral Large | ~1.2s | ~3.2s | ~2.1s |
| Claude Sonnet | ~1.5s | ~4.1s | ~2.8s |
| Llama via OpenRouter | ~0.6s | ~2.1s | ~1.5s |

### Quality Assessment
| Task Type | Best Provider | Quality Score | Alternative |
|-----------|--------------|---------------|-------------|
| Reasoning | Claude | 9.5/10 | Mistral Large (9.2/10) |
| Code | Mistral Codestral | 9.4/10 | Claude (9.1/10) |
| Multilingual | Mistral | 9.3/10 | Claude (9.0/10) |
| Speed | OpenRouter Llama | 8.7/10 | Mistral Small (8.9/10) |

## ğŸ› Common Issues & Solutions

### 1. **API Key Issues**
```bash
Error: 401 Unauthorized
```
**Solution**: Verify API keys are correctly set and active

### 2. **Model Not Found**
```bash
Error: model "xyz" not found  
```
**Solution**: Check exact model names for each provider

### 3. **Rate Limiting**
```bash
Error: 429 Too Many Requests
```
**Solution**: Implement exponential backoff or upgrade plans

### 4. **Provider Compatibility**
```bash
Error: UseResponses not supported
```
**Solution**: Set `UseResponses: param.NewOpt(false)` for non-OpenAI providers

## ğŸ”® Future Roadmap

- ğŸš€ **More Providers**: Google PaLM, Cohere, Together AI
- ğŸ§  **Smart Routing**: Automatic provider selection based on task analysis
- ğŸ“Š **Cost Analytics**: Built-in cost tracking and optimization
- ğŸ”„ **Load Balancing**: Distribute requests across providers
- ğŸ“ˆ **Performance Monitoring**: Real-time provider performance metrics

## ğŸ“š Additional Resources

- [SDK Core Documentation](../../README.md)
- [Agent Patterns Examples](../agent_patterns/)
- [Tool Integration Examples](../tools/)
- [Basic Usage Examples](../basic/)

Choose the provider integration pattern that best fits your use case, performance requirements, and budget constraints. The SDK's flexible architecture makes it easy to switch between providers or use multiple providers simultaneously.